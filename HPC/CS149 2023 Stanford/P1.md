#Cache #Superscalar #Multi-core  #SIMD #Conditional-Instruction 
# Cache
![[Pasted image 20241103035333.png]]

Cold Miss: 
Occurs when data is first accessed and has not yet been loaded into the cache. This miss is due to the fact that the cache has never stored the data in question, so even if the cache size is sufficient, this miss still occurs.
In the figure, a new block of data is loaded without replacing the other blocks.

Capacity Miss: 
Occurs when the cache is full and needs to replace existing data blocks. Even though all data blocks of the cache have been used, there is still not enough space to store all the needed data, thus resulting in a miss.
In the figure, some kind of data is loaded when other data blocks in the current cache must be replaced.


![[Pasted image 20241103145919.png]]
If I want to have Data in DRAM, the data will be loaded in L3, L2 and L1, then the CPU. 
And there are some ways to read the data directly from DRAM. But actually, it still goes through the whole way above. The only difference is that it doesn't store the data in the cache.


Cache is mean to decrease the latency of the machine. If there is no cache, the machine will do the same things as it has cache. But it may be slower.

# The Conditional Instruction in SIMD
![[Pasted image 20241103193151.png]]
![[Pasted image 20241103193205.png]]
# SIMD in CPU and GPU
![[Pasted image 20241104143748.png]]

# Three Different Forms of Parallel Execution

![[Pasted image 20241103190108.png]]
**Superscalar**: This approach exploits instruction-level parallelism (ILP) within a single instruction stream. It processes different instructions from the same stream in parallel within a core. The hardware automatically discovers and manages the parallelism during execution, allowing it to handle multiple instructions at once.

![[Pasted image 20241103190130.png]]
**SIMD (Single Instruction, Multiple Data)**: This method involves multiple arithmetic logic units (ALUs) that are controlled by the same instruction within a core. It is particularly efficient for data-parallel workloads, as it can process multiple data elements in parallel, reducing control costs over many ALUs. Vectorization can be done by the compiler (explicit SIMD) or handled by the hardware at runtime (implicit SIMD).

![[Pasted image 20241103190048.png]]
**Multi-core**: This form of parallelism utilizes multiple processing cores, each capable of running its own instruction stream. It provides thread-level parallelism, meaning it can simultaneously execute completely different instructions on each core. Software can create threads to help expose parallelism to the hardware, often using threading APIs.


![[Pasted image 20241103190700.png]]
This machine uses all the Parallel Methods above.
